{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80679fa2",
   "metadata": {},
   "source": [
    "# Module 02: Data Preprocessing Walkthrough\n",
    "\n",
    "This notebook demonstrates the preprocessing steps for Romanian text, including cleaning, date normalization, and a comparison of different NLP pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10430732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify our setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47ff407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.cleaner import clean_text, parse_romanian_date\n",
    "from preprocessing.nlp_pipeline import RomanianNLP, get_romanian_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc50bf",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning and Date Normalization\n",
    "We use custom logic for cleaning and `dateparser` for robust Romanian date parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec0ec5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: '  În perioada 22 decembrie 2025, comisarii ANPC au aplicat sancțiuni în toată ţara.  '\n",
      "Cleaned:  'În perioada 22 decembrie 2025, comisarii ANPC au aplicat sancțiuni în toată țara.'\n",
      "Date ISO: 2025-12-22\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"  În perioada 22 decembrie 2025, comisarii ANPC au aplicat sancțiuni în toată ţara.  \"\n",
    "cleaned = clean_text(sample_text)\n",
    "date_iso = parse_romanian_date(\"22 decembrie 2025\")\n",
    "\n",
    "print(f\"Original: '{sample_text}'\")\n",
    "print(f\"Cleaned:  '{cleaned}'\")\n",
    "print(f\"Date ISO: {date_iso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2ede4",
   "metadata": {},
   "source": [
    "## 2. Stemming vs Lemmatization\n",
    "\n",
    "**Stemming** is a rule-based process that strips suffixes to find the 'root'.\n",
    "**Lemmatization** is a dictionary-based process that finds the canonical form (lemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35d17813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 20:43:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd16e1124eb4141a8b766e4f2f64f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 20:43:12 INFO: Downloaded file to /home/marius/stanza_resources/resources.json\n",
      "2025-12-22 20:43:12 INFO: Loading these models for language: ro (Romanian):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | rrt          |\n",
      "| pos       | rrt_nocharlm |\n",
      "| lemma     | rrt_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-12-22 20:43:12 INFO: Using device: cpu\n",
      "2025-12-22 20:43:12 INFO: Loading: tokenize\n",
      "2025-12-22 20:43:12 INFO: Loading: pos\n",
      "2025-12-22 20:43:14 INFO: Loading: lemma\n",
      "2025-12-22 20:43:14 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Românii', 'stem': 'român', 'lemma': 'român'},\n",
       " {'text': 'sunt', 'stem': 'sunt', 'lemma': 'fi'},\n",
       " {'text': 'oameni', 'stem': 'oamen', 'lemma': 'om'},\n",
       " {'text': 'ospitalieri', 'stem': 'ospitalier', 'lemma': 'ospitaliar'},\n",
       " {'text': 'și', 'stem': 'și', 'lemma': 'și'},\n",
       " {'text': 'merg', 'stem': 'merg', 'lemma': 'merge'},\n",
       " {'text': 'la', 'stem': 'la', 'lemma': 'la'},\n",
       " {'text': 'munte', 'stem': 'munt', 'lemma': 'munte'},\n",
       " {'text': '.', 'stem': '.', 'lemma': '.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = RomanianNLP()\n",
    "demo_sentence = \"Românii sunt oameni ospitalieri și merg la munte.\"\n",
    "nlp.compare_stemming_vs_lemmatization(demo_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0c953",
   "metadata": {},
   "source": [
    "## 3. Romanian Stopwords\n",
    "Removing frequent words that don't carry much semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6e79a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 356 Romanian stopwords.\n",
      "Top 20: ['a', 'abia', 'acea', 'aceasta', 'această', 'aceea', 'aceeasi', 'acei', 'aceia', 'acel', 'acela', 'acelasi', 'acele', 'acelea', 'acest', 'acesta', 'aceste', 'acestea', 'acestei', 'acestia']\n"
     ]
    }
   ],
   "source": [
    "stopwords = get_romanian_stopwords()\n",
    "print(f\"Found {len(stopwords)} Romanian stopwords.\")\n",
    "print(f\"Top 20: {stopwords[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33001b2d",
   "metadata": {},
   "source": [
    "## 4. Comparative NLP Pipelines\n",
    "Showing how Stanza and NLTK process the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b86c3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stanza Processing:\n",
      "{'text': 'Consumatorii', 'lemma': 'consumator', 'pos': 'NOUN'}\n",
      "{'text': 'au', 'lemma': 'avea', 'pos': 'VERB'}\n",
      "{'text': 'drepturi', 'lemma': 'drept', 'pos': 'NOUN'}\n",
      "{'text': '.', 'lemma': '.', 'pos': 'PUNCT'}\n",
      "\n",
      "SpaCy Processing:\n",
      "{'text': 'Consumatorii', 'lemma': 'consumator', 'pos': 'NOUN'}\n",
      "{'text': 'au', 'lemma': 'avea', 'pos': 'AUX'}\n",
      "{'text': 'drepturi', 'lemma': 'drept', 'pos': 'NOUN'}\n",
      "{'text': '.', 'lemma': '.', 'pos': 'PUNCT'}\n",
      "\n",
      "NLTK Processing (Stemming):\n",
      "{'text': 'Consumatorii', 'stem': 'consum'}\n",
      "{'text': 'au', 'stem': 'au'}\n",
      "{'text': 'drepturi', 'stem': 'dreptur'}\n",
      "{'text': '.', 'stem': '.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStanza Processing:\")\n",
    "stanza_results = nlp.process_with_stanza(\"Consumatorii au drepturi.\")\n",
    "for res in stanza_results:\n",
    "    print(res)\n",
    "\n",
    "print(\"\\nSpaCy Processing:\")\n",
    "spacy_results = nlp.process_with_spacy(\"Consumatorii au drepturi.\")\n",
    "for res in spacy_results:\n",
    "    print(res)\n",
    "\n",
    "print(\"\\nNLTK Processing (Stemming):\")\n",
    "nltk_results = nlp.process_with_nltk(\"Consumatorii au drepturi.\")\n",
    "for res in nltk_results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832943a",
   "metadata": {},
   "source": [
    "## 5. Dataset Processing\n",
    "Finally, we apply our pipeline to the entire dataset.\n",
    "The `process_dataset` function orchestrates cleaning, date parsing, and lemmatization for all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d19caa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Article Example:\n",
      "{\n",
      "  \"url\": \"https://anpc.ro/comandament-anpc-in-zonele-turistice/\",\n",
      "  \"title\": \"Comandament ANPC în zonele turistice\",\n",
      "  \"content\": \"În perioada 15.12.2025–21.12.2025, Autoritatea Națională pentru Protecția Consumatorilor, prin Comandamentul de Iarnă 2025, a verificat activitatea a peste 470 de operatori economici din stațiunile și zonele turistice. ANPC este prezent în toate zonele turistice pe întreaga durată a sezonului sărbătorilor de iarnă, pentru a garanta protecția și drepturile consumato...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from preprocessing.process_dataset import INPUT_FILE, process_dataset\n",
    "\n",
    "# Let's look at one raw article before processing\n",
    "if INPUT_FILE.exists():\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "        if raw_data:\n",
    "            print(\"Raw Article Example:\")\n",
    "            print(json.dumps(raw_data[0], indent=2, ensure_ascii=False)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40e0e2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 20:43:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /home/marius/ore/inlp/gh/inlp/01_data_collection/data/processed/articles_anpc.json...\n",
      "Loaded 237 articles.\n",
      "Initialising Stanza NLP pipeline for Romanian...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5300131cf15d4b26a3d6c96e5766b2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 20:43:15 INFO: Downloaded file to /home/marius/stanza_resources/resources.json\n",
      "2025-12-22 20:43:15 INFO: Loading these models for language: ro (Romanian):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | rrt          |\n",
      "| pos       | rrt_nocharlm |\n",
      "| lemma     | rrt_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-12-22 20:43:15 INFO: Using device: cpu\n",
      "2025-12-22 20:43:15 INFO: Loading: tokenize\n",
      "2025-12-22 20:43:15 INFO: Loading: pos\n",
      "2025-12-22 20:43:16 INFO: Loading: lemma\n",
      "2025-12-22 20:43:17 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [01:41<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Processed 237 articles.\n",
      "Saved to /home/marius/ore/inlp/gh/inlp/02_data_preprocessing/data/processed/articles_anpc_preprocessed.json\n"
     ]
    }
   ],
   "source": [
    "# Run the full pipeline\n",
    "process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c5e1dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Article Example:\n",
      "Title Cleaned: Comandament ANPC în zonele turistice\n",
      "Date ISO: 2025-12-22\n",
      "Tokens (first 5): [{'text': 'În', 'lemma': 'în', 'pos': 'ADP'}, {'text': 'perioada', 'lemma': 'perioadă', 'pos': 'NOUN'}, {'text': '15.12.2025–21.12.2025', 'lemma': '15.12.2025–21.12.2025', 'pos': 'NUM'}, {'text': ',', 'lemma': ',', 'pos': 'PUNCT'}, {'text': 'Autoritatea', 'lemma': 'autoritate', 'pos': 'NOUN'}]\n",
      "Lemmatized Content (snippet): în perioadă 15.12.2025–21.12.2025 autoritate național pentru protecție consumator prin comandament de iarnă 2025 avea verifica activitate al peste 470 de operator economic din stațiune și zonă turisti...\n"
     ]
    }
   ],
   "source": [
    "# Look at the processed result\n",
    "OUTPUT_FILE = Path.cwd().parent / \"data\" / \"processed\" / \"articles_anpc_preprocessed.json\"\n",
    "if OUTPUT_FILE.exists():\n",
    "    with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        processed_data = json.load(f)\n",
    "        if processed_data:\n",
    "            print(\"\\nProcessed Article Example:\")\n",
    "            # Show the new fields added during preprocessing\n",
    "            example = processed_data[0]\n",
    "            print(f\"Title Cleaned: {example.get('title_cleaned')}\")\n",
    "            print(f\"Date ISO: {example.get('date_iso')}\")\n",
    "            print(f\"Tokens (first 5): {example.get('content_tokens')[:5]}\")\n",
    "            print(f\"Lemmatized Content (snippet): {example.get('lemmatized_content')[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
